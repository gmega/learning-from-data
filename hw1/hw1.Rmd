---
title: "Learning From Data - Homework 1"
output: 
  bookdown::html_document2:
    toc: yes
    number_sections: no
---

The source code for this notebook can be found [on Github](https://github.com/gmega/learning-from-data/blob/master/hw1).

```{r message=FALSE}
library(tidyverse)
library(itertools)
library(doParallel)
```

```{r cache=FALSE, echo=FALSE}
knitr::read_chunk('../common/perceptron.R')
knitr::read_chunk('../common/linear-datasets.R')
```

```{r echo=FALSE}
registerDoParallel(cores = parallel::detectCores())
knitr::opts_chunk$set(cache=TRUE)
```

# Exercise 1

* (i) is a direct statistical model which is not learned from data. It is therefore **not learning**;
* (ii) learns from existing labeled data, so it is **supervised learning**;
* (iii) is based on a penalty/reward system, so it is **reinforcement learning**.

Alternative $d$.

# Exercise 2

* (i) is an NP-complete problem so unlikely to be solvable with any model of reasonable complexity (as far as current knowledge goes, anyway);
* (ii) people are already using Machine Learning for this, so definitely a good problem;
* (iii) we have, from physics, a precise model for this already. No need to learn it from data;
* (iv) I would not look at this as an ML problem, but maybe some reinforcement learning model could work here. Plus if I leave this out there would be no correct alternatives to choose from. ;-)

So, (ii) and (iv), alternative $a$.

# Exercise 3

Let $D_1, D_2 \in \{w, b\}$ be the random variables representing the first and second draws, respectively. We want to compute the conditional probability $P[D_2 = b \mid D_1 = b]$:

$$
\begin{align}
P\left[D_2 = b \mid D_1 = b\right] &= \frac{P[D_2 = b \cap D_1 = b]}{P[D_1 = b]} = \\
&= \frac{\frac{1}{2} \cdot 1 \cdot 1 + \frac{1}{2}\cdot \frac{1}{2} \cdot 0}{\frac{1}{2}\cdot 1 + \frac{1}{2}\cdot\frac{1}{2}} =\\ 
&= \frac{\frac{1}{2}}{\frac{3}{4}} = \frac{2}{3}
\end{align}
$$

And this corresponds to answer $d$.

# Exercise 4 {#sec:exercise-4}

The probability of getting no red marbles is given by the probability of failing $10$ bernoulli trials with $p = 1 - 0.55$. We get, therefore:

```{r}
p <- (1 - 0.55)**10
formatC(p, format = 'e', digits = 3)
```

So answer $b$.

# Exercise 5

This is given by $1$ minus that probability that each and every sample has at least one red marble in it. The probability that a sample has at least one red marble, in turn, is given by $1 - p$, where $p$ is the probability that we calculated for [Exercise 4](#sec:exercise-4). Therefore:

```{r}
round(1 - (1 - p)**1000, digits = 3)
```

And that corresponds to answer $c$.

# Exercises 7-10: Linear Perceptron

For the next exercises we will need a linear perceptron. We start by coding one up:

```{r pla}
```

We will also need code to generate the initial data points scattered over the $[-1, 1]$ square, and the initial linearly separable dataset:

```{r linear-dataset-generators}
```

## Exercise 7

We run $1000$ sims and look at the statistics for convergence time.

```{r}
summary(
  unlist(mclapply(
    1:1000,
    function(x) {
      D <- generate_ls_dataset(n = 10)  
      length(pla(
        X = D$X, 
        z = D$z, 
        w = c(0, 0, 0)
      ))
    },
    mc.cores = detectCores()
  ))
)
```

My perceptron converges in $\sim 10$ iterations, on average, which is closer to $15$ than it is to $5$, so we get answer $b$. We plot runs of the perceptron in Figures \@ref(fig:perceptron-10) and \@ref(fig:perceptron-400), and they indeed seem to converge to the right boundary.

```{r linear-dataset-plotting, echo=FALSE}
```

```{r pla-plot, echo=FALSE}
```

```{r perceptron-10, fig.cap='Perceptron after convergence with 10 data points.', cache = FALSE}
D <- generate_ls_dataset(n = 10)
plot_pla(
  D, pla(D$X, D$z), last_only = FALSE
)
```

```{r perceptron-400, fig.cap='Perceptron after convergence with 400 data points.'}
D <- generate_ls_dataset(n = 400)
plot_pla(
  D, pla(D$X, D$z), last_only = FALSE
)
```

## Exercise 8

We will shoot points at the board and see how many fall between $f$ and $g$.

```{r}
error_sim <- function(t, g, n) {
  X <- generate_points(n, -1, 1)
  sum(f(X, t) != f(X, g)) / n
}
```

We will run $1000$ trials where we compute the percentage of disagreeing pairs for batches of $100\,000$ points. We then average those together. Averages-of-averages are fine as long as the samples are of the same size:

$$
\frac{\frac{1}{n}\sum_{i=1}^n s_i + \frac{1}{n}\sum_{i=n+1}^{2n} s_i}{2} = \frac{\frac{1}{n}\sum_{i=1}^{2n}s_i }{2}
 = \frac{1}{2n}\sum_{i=1}^{2n}s_i 
$$

And here we go.

```{r}
summary(
  unlist(mclapply(
    1:1000,
    function(x) {
      D <- generate_ls_dataset(n = 10) 
      error_sim(
        t = D$w, 
        g = pla_line(unlist(pla(D$X, D$z) %>% tail(1))), 
        n = 100000
      )
    },
     mc.cores = detectCores()
  ))
)
```

So answer $c$, as the mean is closer to $0.1$ than it is to $0.5$.

## Exercise 9

```{r}
summary(
  unlist(mclapply(
    1:1000,
    function(x) {
      D <- generate_ls_dataset(n = 100)  
      length(pla(
        X = D$X, 
        z = D$z, 
        w = c(0, 0, 0)
      ))
    },
    mc.cores = detectCores()
  ))
)
```

So answer $b$, as the mean is closer to $100$ than it is to $500$.

## Exercise 10

We run the simulation as before.

```{r}
summary(
  unlist(mclapply(
    1:1000,
    function(x) {
      D <- generate_ls_dataset(n = 100) 
      error_sim(
        t = D$w, 
        g = pla_line(unlist(pla(D$X, D$z) %>% tail(1))), 
        n = 100000
      )
    },
    mc.cores = detectCores()
  ))
)
```

So answer $b$, as the mean is closer to $0.01$ than it is to $0.1$.
